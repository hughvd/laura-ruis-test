{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use in a colab notebook with T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS8Qbq00vx8Z",
        "outputId": "d91991c9-55d9-4870-a8a4-d73221d374d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'laura-ruis-test'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 21 (delta 6), reused 19 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (21/21), 117.48 KiB | 23.50 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hughvd/laura-ruis-test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBz9WSyLwLw7",
        "outputId": "f2a2a4a3-57f3-43ea-e0e8-d95ef13e849d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/laura-ruis-test\n"
          ]
        }
      ],
      "source": [
        "%cd laura-ruis-test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O4PEf4ytwOUw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_API_KEY\"] = \"key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JdzNMnuuyYdh",
        "outputId": "12d6efc1-4beb-4a48-d7dc-f0a6aeabbf23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.47.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch transformers wandb numpy pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9vzU9Rb1Bga",
        "outputId": "ea13ff86-b762-4f3d-ac04-ce2bafaf912b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating train + val\n",
            "Generating test sets\n",
            "Generating test_holdout_a\n",
            "Generating test_4digit\n",
            "Generating test_leading_zero\n",
            "Datasets written to: /content/laura-ruis-test/data\n"
          ]
        }
      ],
      "source": [
        "!python generate_data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PeyI6ASydxe",
        "outputId": "3f6ff631-8199-4c6d-90e4-4070411874d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-01-03 09:23:10.940655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1767432190.975007   10514 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1767432190.986663   10514 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1767432191.013543   10514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767432191.013582   10514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767432191.013591   10514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767432191.013599   10514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-03 09:23:11.021475: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhughvandeventer\u001b[0m (\u001b[33mhughvandeventer-harvard-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m setting up run z088vm4g (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run z088vm4g (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run z088vm4g (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/laura-ruis-test/wandb/run-20260103_092330-z088vm4g\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlsd-addition-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/hughvandeventer-harvard-university/laura-ruis-test\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/hughvandeventer-harvard-university/laura-ruis-test/runs/z088vm4g\u001b[0m\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
            "step 200 | train_loss 1.6458 | val_loss 1.6083\n",
            "step 400 | train_loss 1.5431 | val_loss 1.5330\n",
            "step 600 | train_loss 1.3689 | val_loss 1.3443\n",
            "step 800 | train_loss 1.2594 | val_loss 1.2270\n",
            "step 1000 | train_loss 1.0852 | val_loss 0.9728\n",
            "step 1200 | train_loss 0.8562 | val_loss 0.7635\n",
            "step 1400 | train_loss 0.7315 | val_loss 0.6724\n",
            "step 1600 | train_loss 0.7124 | val_loss 0.6234\n",
            "step 1800 | train_loss 0.6058 | val_loss 0.4910\n",
            "step 2000 | train_loss 0.2998 | val_loss 0.1658\n",
            "step 2200 | train_loss 0.2005 | val_loss 0.1097\n",
            "step 2400 | train_loss 0.2162 | val_loss 0.0698\n",
            "step 2600 | train_loss 0.1702 | val_loss 0.0515\n",
            "step 2800 | train_loss 0.1313 | val_loss 0.0369\n",
            "step 3000 | train_loss 0.1217 | val_loss 0.0259\n",
            "step 3200 | train_loss 0.0860 | val_loss 0.0255\n",
            "step 3400 | train_loss 0.0665 | val_loss 0.0192\n",
            "step 3600 | train_loss 0.0499 | val_loss 0.0149\n",
            "step 3800 | train_loss 0.0562 | val_loss 0.0126\n",
            "step 4000 | train_loss 0.0375 | val_loss 0.0090\n",
            "step 4200 | train_loss 0.1018 | val_loss 0.0101\n",
            "step 4400 | train_loss 0.0525 | val_loss 0.0087\n",
            "step 4600 | train_loss 0.0609 | val_loss 0.0065\n",
            "step 4800 | train_loss 0.0326 | val_loss 0.0068\n",
            "step 5000 | train_loss 0.0309 | val_loss 0.0053\n",
            "step 5200 | train_loss 0.0463 | val_loss 0.0036\n",
            "step 5400 | train_loss 0.0411 | val_loss 0.0035\n",
            "step 5600 | train_loss 0.0205 | val_loss 0.0040\n",
            "step 5800 | train_loss 0.0363 | val_loss 0.0027\n",
            "step 6000 | train_loss 0.0166 | val_loss 0.0027\n",
            "step 6200 | train_loss 0.0278 | val_loss 0.0026\n",
            "step 6400 | train_loss 0.0264 | val_loss 0.0019\n",
            "step 6600 | train_loss 0.0089 | val_loss 0.0016\n",
            "step 6800 | train_loss 0.0064 | val_loss 0.0017\n",
            "step 7000 | train_loss 0.0293 | val_loss 0.0013\n",
            "step 7200 | train_loss 0.0512 | val_loss 0.0015\n",
            "step 7400 | train_loss 0.0301 | val_loss 0.0012\n",
            "step 7600 | train_loss 0.0128 | val_loss 0.0013\n",
            "step 7800 | train_loss 0.0059 | val_loss 0.0008\n",
            "step 8000 | train_loss 0.0236 | val_loss 0.0008\n",
            "step 8200 | train_loss 0.0056 | val_loss 0.0007\n",
            "step 8400 | train_loss 0.0158 | val_loss 0.0007\n",
            "step 8600 | train_loss 0.0092 | val_loss 0.0006\n",
            "step 8800 | train_loss 0.0170 | val_loss 0.0005\n",
            "step 9000 | train_loss 0.0077 | val_loss 0.0005\n",
            "step 9200 | train_loss 0.0032 | val_loss 0.0004\n",
            "step 9400 | train_loss 0.0224 | val_loss 0.0006\n",
            "step 9600 | train_loss 0.0063 | val_loss 0.0003\n",
            "step 9800 | train_loss 0.0024 | val_loss 0.0003\n",
            "step 10000 | train_loss 0.0140 | val_loss 0.0003\n",
            "step 10200 | train_loss 0.0047 | val_loss 0.0005\n",
            "step 10400 | train_loss 0.0056 | val_loss 0.0003\n",
            "step 10600 | train_loss 0.0032 | val_loss 0.0004\n",
            "step 10800 | train_loss 0.0017 | val_loss 0.0004\n",
            "step 11000 | train_loss 0.0076 | val_loss 0.0003\n",
            "step 11200 | train_loss 0.0141 | val_loss 0.0002\n",
            "step 11400 | train_loss 0.0153 | val_loss 0.0003\n",
            "step 11600 | train_loss 0.0067 | val_loss 0.0002\n",
            "step 11800 | train_loss 0.0117 | val_loss 0.0002\n",
            "step 12000 | train_loss 0.0018 | val_loss 0.0002\n",
            "step 12200 | train_loss 0.0138 | val_loss 0.0002\n",
            "step 12400 | train_loss 0.0085 | val_loss 0.0002\n",
            "step 12600 | train_loss 0.0011 | val_loss 0.0001\n",
            "step 12800 | train_loss 0.0012 | val_loss 0.0001\n",
            "step 13000 | train_loss 0.0064 | val_loss 0.0002\n",
            "step 13200 | train_loss 0.0124 | val_loss 0.0002\n",
            "step 13400 | train_loss 0.0012 | val_loss 0.0001\n",
            "step 13600 | train_loss 0.0012 | val_loss 0.0001\n",
            "step 13800 | train_loss 0.0077 | val_loss 0.0001\n",
            "step 14000 | train_loss 0.0007 | val_loss 0.0001\n",
            "step 14200 | train_loss 0.0019 | val_loss 0.0001\n",
            "step 14400 | train_loss 0.0063 | val_loss 0.0002\n",
            "step 14600 | train_loss 0.0011 | val_loss 0.0001\n",
            "step 14800 | train_loss 0.0022 | val_loss 0.0001\n",
            "step 15000 | train_loss 0.0023 | val_loss 0.0001\n",
            "step 15200 | train_loss 0.0016 | val_loss 0.0001\n",
            "step 15400 | train_loss 0.0003 | val_loss 0.0001\n",
            "step 15600 | train_loss 0.0057 | val_loss 0.0001\n",
            "step 15800 | train_loss 0.0008 | val_loss 0.0001\n",
            "step 16000 | train_loss 0.0005 | val_loss 0.0001\n",
            "step 16200 | train_loss 0.0010 | val_loss 0.0001\n",
            "step 16400 | train_loss 0.0009 | val_loss 0.0001\n",
            "step 16600 | train_loss 0.0065 | val_loss 0.0001\n",
            "step 16800 | train_loss 0.0004 | val_loss 0.0001\n",
            "step 17000 | train_loss 0.0009 | val_loss 0.0002\n",
            "step 17200 | train_loss 0.0004 | val_loss 0.0001\n",
            "step 17400 | train_loss 0.0025 | val_loss 0.0001\n",
            "step 17600 | train_loss 0.0011 | val_loss 0.0001\n",
            "step 17800 | train_loss 0.0011 | val_loss 0.0001\n",
            "step 18000 | train_loss 0.0008 | val_loss 0.0000\n",
            "step 18200 | train_loss 0.0043 | val_loss 0.0001\n",
            "step 18400 | train_loss 0.0007 | val_loss 0.0001\n",
            "step 18600 | train_loss 0.0006 | val_loss 0.0001\n",
            "step 18800 | train_loss 0.0037 | val_loss 0.0000\n",
            "step 19000 | train_loss 0.0014 | val_loss 0.0000\n",
            "step 19200 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 19400 | train_loss 0.0009 | val_loss 0.0000\n",
            "step 19600 | train_loss 0.0020 | val_loss 0.0001\n",
            "step 19800 | train_loss 0.0005 | val_loss 0.0000\n",
            "step 20000 | train_loss 0.0012 | val_loss 0.0000\n",
            "step 20200 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 20400 | train_loss 0.0006 | val_loss 0.0000\n",
            "step 20600 | train_loss 0.0016 | val_loss 0.0000\n",
            "step 20800 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 21000 | train_loss 0.0010 | val_loss 0.0000\n",
            "step 21200 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 21400 | train_loss 0.0014 | val_loss 0.0000\n",
            "step 21600 | train_loss 0.0004 | val_loss 0.0000\n",
            "step 21800 | train_loss 0.0009 | val_loss 0.0000\n",
            "step 22000 | train_loss 0.0015 | val_loss 0.0000\n",
            "step 22200 | train_loss 0.0040 | val_loss 0.0000\n",
            "step 22400 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 22600 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 22800 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 23000 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 23200 | train_loss 0.0018 | val_loss 0.0000\n",
            "step 23400 | train_loss 0.0004 | val_loss 0.0000\n",
            "step 23600 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 23800 | train_loss 0.0166 | val_loss 0.0000\n",
            "step 24000 | train_loss 0.0009 | val_loss 0.0000\n",
            "step 24200 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 24400 | train_loss 0.0006 | val_loss 0.0000\n",
            "step 24600 | train_loss 0.0004 | val_loss 0.0000\n",
            "step 24800 | train_loss 0.0009 | val_loss 0.0000\n",
            "step 25000 | train_loss 0.0007 | val_loss 0.0000\n",
            "step 25200 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 25400 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 25600 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 25800 | train_loss 0.0013 | val_loss 0.0000\n",
            "step 26000 | train_loss 0.0227 | val_loss 0.0000\n",
            "step 26200 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 26400 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 26600 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 26800 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 27000 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 27200 | train_loss 0.0004 | val_loss 0.0000\n",
            "step 27400 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 27600 | train_loss 0.0007 | val_loss 0.0000\n",
            "step 27800 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 28000 | train_loss 0.0012 | val_loss 0.0000\n",
            "step 28200 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 28400 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 28600 | train_loss 0.0004 | val_loss 0.0000\n",
            "step 28800 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 29000 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 29200 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 29400 | train_loss 0.0012 | val_loss 0.0000\n",
            "step 29600 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 29800 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 30000 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 30200 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 30400 | train_loss 0.0004 | val_loss 0.0000\n",
            "step 30600 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 30800 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 31000 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 31200 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 31400 | train_loss 0.0006 | val_loss 0.0000\n",
            "step 31600 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 31800 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 32000 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 32200 | train_loss 0.0056 | val_loss 0.0000\n",
            "step 32400 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 32600 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 32800 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 33000 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 33200 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 33400 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 33600 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 33800 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 34000 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 34200 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 34400 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 34600 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 34800 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 35000 | train_loss 0.0005 | val_loss 0.0000\n",
            "step 35200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 35400 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 35600 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 35800 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 36000 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 36200 | train_loss 0.0005 | val_loss 0.0000\n",
            "step 36400 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 36600 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 36800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 37000 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 37200 | train_loss 0.0050 | val_loss 0.0000\n",
            "step 37400 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 37600 | train_loss 0.0011 | val_loss 0.0000\n",
            "step 37800 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 38000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 38200 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 38400 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 38600 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 38800 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 39000 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 39200 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 39400 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 39600 | train_loss 0.0009 | val_loss 0.0000\n",
            "step 39800 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 40000 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 40200 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 40400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 40600 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 40800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 41000 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 41200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 41400 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 41600 | train_loss 0.0225 | val_loss 0.0000\n",
            "step 41800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 42000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 42200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 42400 | train_loss 0.0039 | val_loss 0.0000\n",
            "step 42600 | train_loss 0.0026 | val_loss 0.0000\n",
            "step 42800 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 43000 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 43200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 43400 | train_loss 0.0003 | val_loss 0.0000\n",
            "step 43600 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 43800 | train_loss 0.0009 | val_loss 0.0000\n",
            "step 44000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 44200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 44400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 44600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 44800 | train_loss 0.0017 | val_loss 0.0000\n",
            "step 45000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 45200 | train_loss 0.0002 | val_loss 0.0000\n",
            "step 45400 | train_loss 0.0009 | val_loss 0.0000\n",
            "step 45600 | train_loss 0.0224 | val_loss 0.0000\n",
            "step 45800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 46000 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 46200 | train_loss 0.0004 | val_loss 0.0000\n",
            "step 46400 | train_loss 0.0092 | val_loss 0.0000\n",
            "step 46600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 46800 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 47000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 47200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 47400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 47600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 47800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 48000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 48200 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 48400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 48600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 48800 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 49000 | train_loss 0.0019 | val_loss 0.0000\n",
            "step 49200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 49400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 49600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 49800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 50000 | train_loss 0.0009 | val_loss 0.0000\n",
            "step 50200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 50400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 50600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 50800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 51000 | train_loss 0.0004 | val_loss 0.0000\n",
            "step 51200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 51400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 51600 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 51800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 52000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 52200 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 52400 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 52600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 52800 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 53000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 53200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 53400 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 53600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 53800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 54000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 54200 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 54400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 54600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 54800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 55000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 55200 | train_loss 0.0001 | val_loss 0.0000\n",
            "step 55400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 55600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 55800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 56000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 56200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 56400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 56600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 56800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 57000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 57200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 57400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 57600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 57800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 58000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 58200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 58400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 58600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 58800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 59000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 59200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 59400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 59600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 59800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 60000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 60200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 60400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 60600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 60800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 61000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 61200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 61400 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 61600 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 61800 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 62000 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 62200 | train_loss 0.0000 | val_loss 0.0000\n",
            "step 62400 | train_loss 0.0000 | val_loss 0.0000\n",
            "Saved raw generations to data/results_raw.jsonl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading output.log 15.0KB/15.0KB (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading wandb-summary.json 237B/237B (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading output.log 15.0KB/15.0KB (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading wandb-summary.json 237B/237B (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading output.log 15.0KB/15.0KB (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading wandb-summary.json 237B/237B (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading output.log 15.0KB/15.0KB (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading wandb-summary.json 237B/237B (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading output.log 15.0KB/15.0KB (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading wandb-summary.json 237B/237B (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading output.log 15.0KB/15.0KB (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading wandb-summary.json 237B/237B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading config.yaml 2.7KB/2.7KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading output.log 15.0KB/15.0KB (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading wandb-summary.json 237B/237B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading config.yaml 2.7KB/2.7KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading output.log 15.0KB/15.0KB (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading wandb-summary.json 237B/237B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading config.yaml 2.7KB/2.7KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading output.log 15.0KB/15.0KB (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading wandb-summary.json 237B/237B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading config.yaml 2.7KB/2.7KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading output.log 15.0KB/15.0KB (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading wandb-summary.json 237B/237B (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading config.yaml 2.7KB/2.7KB (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading history steps 62499-62499, summary, console lines 313-313 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading history steps 62499-62499, summary, console lines 313-313 ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/grad_norm ‚ñà‚ñà‚ñà‚ñÖ‚ñá‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/lr ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            step 62500\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/grad_norm 0.00046\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/loss 1e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/lr 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/loss 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlsd-addition-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/hughvandeventer-harvard-university/laura-ruis-test/runs/z088vm4g\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/hughvandeventer-harvard-university/laura-ruis-test\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260103_092330-z088vm4g/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_zQ2KuvzaXZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
